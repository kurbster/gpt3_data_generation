defaults:
  - outputs
  - preprocessing_config: wic
  - metric: f1_score
  - _self_
hydra:
  env:
    model_name: "bert"
    dataset_name: "wic"
    save_name: "${hydra:env.dataset_name}_${hydra:env.model_name}"

do_train: True
do_eval: True
do_predict: True

model_name: "${hydra:env.model_name}"

train_file: "datasets/${hydra:env.dataset_name}/generated/gpt3_generated.json"
dataset_name: "${hydra:env.dataset_name}"

cache_dir: "./.cache"
output_dir: "models/${hydra:env.dataset_name}/${hydra:env.model_name}"

text_column: "text"
label_column: "label"

save_strategy: "epoch"
evaluation_strategy: "epoch"

num_samples: 410

save_total_limit: 1          # Save a max of 1 epoch
preprocessing_num_workers: 4
num_labels: 2

metric_for_best_model: "accuracy"
load_best_model_at_end: True # Load the best model at the end of training

seed: 42
per_device_eval_batch_size: 32
per_device_train_batch_size: 4  # To match the 8 batch size of eariler experiments

# Set to true if on a TPU
#pad_to_max_length: True

warmup_ratio: .20 # This would match the 11 epochs in eariler experiments
learning_rate: .0000005 # 5e-7
num_train_epochs: 200
optim: "adamw_torch"

early_stopping_patience: 75
early_stopping_threshold: 0.0001
